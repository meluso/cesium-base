#!/bin/bash

#-------------------  Begin SLURM preamble  -------------------------#
#SBATCH --job-name=cesium_test
#SBATCH --partition=short
#SBATCH --ntasks=1
#SBATCH --mem-per-cpu=4000mb
#SBATCH --time=0-00:30:00
#SBATCH --mail-type=ALL
#SBATCH --output=/gpfs1/home/j/m/%u/cesium/logs/test/%x_%j.log
#-------------------   End SLURM preamble   -------------------------#

# Specify execution number
EXECNUM=10

# Specify log and data directorires
EXECFMT=$(printf "%03d" $EXECNUM)
LOGDIR=/gpfs1/home/j/m/jmeluso/cesium/logs/test
JOBDIR=/gpfs2/scratch/jmeluso/cesium/data/test

# Make log directory
if [ ! -d $LOGDIR ] ; then
	mkdir $LOGDIR
fi

# Make job directory
if [ ! -d $JOBDIR ] ; then
	mkdir $JOBDIR
fi

# Move to model directory
cd /gpfs1/home/j/m/jmeluso/cesium/model

# Echo some useful and interesting information
echo "  running host:    ${SLURMD_NODENAME}"
echo "  assigned nodes:  ${SLURM_JOB_NODELIST}"
echo "  partition used:  ${SLURM_JOB_PARTITION}"
echo "  jobid:           ${SLURM_JOBID}"
echo ""

# Load python module
spack load python@3.7.7
spack load py-numpy@1.18.4
spack load py-networkx@2.4

# Run simulation in python for job ii
python run_simulation.py $JOBDIR $EXECNUM "${ii}"


# to submit 100 jobs, call the file submit_loop.sh
# to run live, use the following command:
#   srun --partition=bluemoon --ntasks=1 --mem=4G --time=4:00:00 --pty /bin/bash